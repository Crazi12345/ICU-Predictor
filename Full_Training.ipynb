{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ICU Sepsis Prediction - Full Training Pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import argparse\n",
    "import pickle\n",
    "import json\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate timestamp for plot directory\n",
    "run_timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "plot_dir = f\"plotsOff_{run_timestamp}\"\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    average_precision_score,\n",
    "    roc_curve,\n",
    "    precision_recall_curve,\n",
    "    confusion_matrix,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    ")\n",
    "\n",
    "\n",
    "# Configure logging (Modified for Notebook)\n",
    "import sys\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s', handlers=[logging.StreamHandler(sys.stdout)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "logging.info(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "# --- Notebook Configuration ---\n",
    "# Replaces command line arguments\n",
    "class Args:\n",
    "    use_cache = True\n",
    "    use_last_final = False\n",
    "    use_all_pp = True\n",
    "\n",
    "args = Args()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Configs\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def get_last_final_configs():\n",
    "    \"\"\"Returns the best configurations from the last run (JSON), or falls back to hardcoded.\"\"\"\n",
    "    json_path = \"best_model_configs.json\"\n",
    "    if os.path.exists(json_path):\n",
    "        try:\n",
    "            with open(json_path, 'r') as f:\n",
    "                configs = json.load(f)\n",
    "            \n",
    "            # Infer model_type if missing\n",
    "            valid_configs = []\n",
    "            for c in configs:\n",
    "                if 'model_type' not in c:\n",
    "                    if 'RNN' in c['name']:\n",
    "                        c['model_type'] = 'RNN'\n",
    "                    elif 'CNN' in c['name']:\n",
    "                        c['model_type'] = 'CNN'\n",
    "                    elif 'LGSTM' in c['name']:\n",
    "                        c['model_type'] = 'LGSTM'\n",
    "                    else:\n",
    "                        logging.warning(f\"Could not infer model_type for {c['name']}, skipping.\")\n",
    "                        continue\n",
    "                valid_configs.append(c)\n",
    "\n",
    "            logging.info(f\"Loaded {len(valid_configs)} configurations from {json_path}\")\n",
    "            return valid_configs\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Failed to load {json_path}: {e}\")\n",
    "\n",
    "    logging.info(\"Using HARDCODED fallback configurations.\")\n",
    "    return [\n",
    "        {\n",
    "            'name': 'RNN_BestUtil_82',\n",
    "            'model_type': 'RNN',\n",
    "            'hp': {'batch_size': 32, 'dropout': 0.4, 'final_activation': 'leakyrelu', 'loss': 'mse', 'lr': 0.001, 'optimizer': 'adamw', 'units': 64},\n",
    "            'score_type': 'utility',\n",
    "            'val_score': 0.0980\n",
    "        },\n",
    "        {\n",
    "            'name': 'RNN_BestUtil_144',\n",
    "            'model_type': 'RNN',\n",
    "            'hp': {'batch_size': 64, 'dropout': 0.2, 'final_activation': 'leakyrelu', 'loss': 'mse', 'lr': 0.001, 'optimizer': 'adam', 'units': 64},\n",
    "            'score_type': 'utility',\n",
    "            'val_score': 0.0792\n",
    "        },\n",
    "        {\n",
    "            'name': 'RNN_BestAUC_65',\n",
    "            'model_type': 'RNN',\n",
    "            'hp': {'batch_size': 32, 'dropout': 0.4, 'final_activation': 'sigmoid', 'loss': 'binary_crossentropy', 'lr': 0.001, 'optimizer': 'adam', 'units': 128},\n",
    "            'score_type': 'auc',\n",
    "            'val_score': 0.9895\n",
    "        },\n",
    "        {\n",
    "            'name': 'RNN_BestAUC_3',\n",
    "            'model_type': 'RNN',\n",
    "            'hp': {'batch_size': 32, 'dropout': 0.2, 'final_activation': 'sigmoid', 'loss': 'binary_crossentropy', 'lr': 0.001, 'optimizer': 'adamw', 'units': 128},\n",
    "            'score_type': 'auc',\n",
    "            'val_score': 0.9888\n",
    "        },\n",
    "        {\n",
    "            'name': 'CNN_BestUtil_1462',\n",
    "            'model_type': 'CNN',\n",
    "            'hp': {'batch_size': 64, 'dropout': 0.2, 'f1': 64, 'f2': 128, 'final_activation': 'leakyrelu', 'kernel_size': 5, 'loss': 'mse', 'lr': 0.0005, 'optimizer': 'adamw', 'stride': 1},\n",
    "            'score_type': 'utility',\n",
    "            'val_score': 0.0794\n",
    "        },\n",
    "        {\n",
    "            'name': 'CNN_BestUtil_386',\n",
    "            'model_type': 'CNN',\n",
    "            'hp': {'batch_size': 32, 'dropout': 0.2, 'f1': 64, 'f2': 128, 'final_activation': 'sigmoid', 'kernel_size': 3, 'loss': 'binary_crossentropy', 'lr': 0.001, 'optimizer': 'adamw', 'stride': 1},\n",
    "            'score_type': 'utility',\n",
    "            'val_score': 0.0744\n",
    "        },\n",
    "        {\n",
    "            'name': 'CNN_BestAUC_658',\n",
    "            'model_type': 'CNN',\n",
    "            'hp': {'batch_size': 32, 'dropout': 0.4, 'f1': 32, 'f2': 128, 'final_activation': 'sigmoid', 'kernel_size': 5, 'loss': 'binary_crossentropy', 'lr': 0.001, 'optimizer': 'adamw', 'stride': 1},\n",
    "            'score_type': 'auc',\n",
    "            'val_score': 0.9919\n",
    "        },\n",
    "        {\n",
    "            'name': 'CNN_BestAUC_1138',\n",
    "            'model_type': 'CNN',\n",
    "            'hp': {'batch_size': 64, 'dropout': 0.2, 'f1': 32, 'f2': 64, 'final_activation': 'tanh', 'kernel_size': 5, 'loss': 'mse', 'lr': 0.001, 'optimizer': 'adamw', 'stride': 1},\n",
    "            'score_type': 'auc',\n",
    "            'val_score': 0.9915\n",
    "        },\n",
    "        {\n",
    "            'name': 'LGSTM_BestUtil_0',\n",
    "            'model_type': 'LGSTM',\n",
    "            'hp': {'batch_size': 8, 'dropout': 0.2, 'final_activation': 'sigmoid', 'loss': 'binary_crossentropy', 'lr': 0.001, 'optimizer': 'adam', 'u1': 64, 'u2': 32},\n",
    "            'score_type': 'utility',\n",
    "            'val_score': 0.1044\n",
    "        },\n",
    "        {\n",
    "            'name': 'LGSTM_BestUtil_1',\n",
    "            'model_type': 'LGSTM',\n",
    "            'hp': {'batch_size': 8, 'dropout': 0.2, 'final_activation': 'sigmoid', 'loss': 'binary_crossentropy', 'lr': 0.001, 'optimizer': 'adam', 'u1': 64, 'u2': 64},\n",
    "            'score_type': 'utility',\n",
    "            'val_score': -999.0000\n",
    "        }\n",
    "    ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def get_data(use_cache=False, cache_path='preprocessed_data.pkl'):\n",
    "    if use_cache and os.path.exists(cache_path):\n",
    "        logging.info(f\"Loading preprocessed data from {cache_path}...\")\n",
    "        try:\n",
    "            with open(cache_path, 'rb') as f:\n",
    "                data = pickle.load(f)\n",
    "            logging.info(\"\u2713 Data loaded from cache.\")\n",
    "            return data\n",
    "        except Exception as e:\n",
    "            logging.info(f\"\u26a0 Failed to load cache ({e}). Re-running preprocessing...\")\n",
    "\n",
    "    # If we are here, and a specific cache_path was requested (e.g., preprocessed_mean.pkl),\n",
    "    # but it doesn't exist, we can't just fall back to the default pipeline easily because\n",
    "    # the default pipeline in this script does the \"neg1\" strategy (the one we hardcoded earlier).\n",
    "    # Ideally, preprocessing.py should be used to generate these files.\n",
    "\n",
    "    if cache_path != 'preprocessed_data.pkl':\n",
    "         logging.error(f\"Cache file {cache_path} not found! Please run preprocessing.py first.\")\n",
    "         sys.exit(1)\n",
    "\n",
    "    # --- Default Data Loading (Fallback) ---\n",
    "    try:\n",
    "        import kagglehub\n",
    "        path = kagglehub.dataset_download(\"salikhussaini49/prediction-of-sepsis\")\n",
    "        logging.info(f\"Dataset downloaded to: {path}\")\n",
    "        dataset_path = path\n",
    "    except Exception as e:\n",
    "        logging.info(f\"Download failed ({e}). Using local path or expecting Dataset.csv in working dir.\")\n",
    "        dataset_path = os.getcwd()\n",
    "\n",
    "    possible_paths = [\n",
    "        os.path.join(dataset_path, \"Dataset.csv\"),\n",
    "        os.path.join(dataset_path, \"dataset.csv\"),\n",
    "        os.path.join(os.path.expanduser(\"~\"), \"Downloads\", \"Dataset.csv\"),\n",
    "        os.path.join(os.path.expanduser(\"~\"), \"Downloads\", \"dataset.csv\"),\n",
    "        \"Dataset.csv\",\n",
    "        \"dataset.csv\",\n",
    "    ]\n",
    "\n",
    "    csv_file = None\n",
    "    for path in possible_paths:\n",
    "        if os.path.exists(path):\n",
    "            csv_file = path\n",
    "            logging.info(f\"\u2713 Found dataset at: {csv_file}\")\n",
    "            break\n",
    "\n",
    "    df_full = None\n",
    "    if csv_file:\n",
    "        df_full = pd.read_csv(csv_file)\n",
    "        logging.info(f\"\u2713 Loaded Dataset: {df_full.shape[0]} rows, {df_full.shape[1]} columns\")\n",
    "    else:\n",
    "        # Synthetic fallback\n",
    "        logging.info(\"\\n\u26a0 Creating synthetic dataset...\")\n",
    "        n_patients = 100\n",
    "        max_time_steps = 200\n",
    "        n_features = 15\n",
    "        data_list = []\n",
    "        for p_id in range(n_patients):\n",
    "            n_steps = np.random.randint(50, max_time_steps)\n",
    "            for t in range(n_steps):\n",
    "                features = np.random.randn(n_features) * 0.5\n",
    "                has_sepsis = np.random.rand() > 0.7\n",
    "                if has_sepsis:\n",
    "                    features[:3] += t / max_time_steps * 2\n",
    "                sepsis_label = 1 if (has_sepsis and t > n_steps * 0.6) else 0\n",
    "                data_list.append({'patient_id': p_id, **{f'feature_{i}': features[i] for i in range(n_features)}, 'SepsisLabel': sepsis_label})\n",
    "        df_full = pd.DataFrame(data_list)\n",
    "\n",
    "    # --- Preprocessing ---\n",
    "    if df_full is not None:\n",
    "        if 'Unnamed: 0' in df_full.columns:\n",
    "            df_full = df_full.drop(columns=['Unnamed: 0'])\n",
    "        if 'Patient_ID' in df_full.columns:\n",
    "            df_full.rename(columns={'Patient_ID': 'patient_id'}, inplace=True)\n",
    "\n",
    "        logging.info(f\"Shape: {df_full.shape}\")\n",
    "\n",
    "        df_imputed = df_full.copy()\n",
    "        numeric_cols = df_imputed.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        if 'patient_id' in numeric_cols: numeric_cols.remove('patient_id')\n",
    "        if 'SepsisLabel' in numeric_cols: numeric_cols.remove('SepsisLabel')\n",
    "\n",
    "        # Per-patient imputation and scaling\n",
    "        logging.info(\"\u23f3 Imputing (ffill/bfill) and Scaling per-patient (ignoring NaNs)...\")\n",
    "        df_scaled = df_imputed.copy()\n",
    "\n",
    "        # 1. Temporal Imputation\n",
    "        for col in numeric_cols:\n",
    "            df_scaled[col] = df_scaled.groupby('patient_id')[col].transform(lambda x: x.ffill().bfill())\n",
    "\n",
    "        # 2. Scaling ignoring NaNs\n",
    "        def robust_scale_ignore_nan(x):\n",
    "            if x.isna().all():\n",
    "                return x\n",
    "            mean = np.nanmean(x)\n",
    "            std = np.nanstd(x)\n",
    "            if np.isnan(std) or std == 0:\n",
    "                return x - mean if not np.isnan(mean) else x\n",
    "            return (x - mean) / std\n",
    "\n",
    "        for col in numeric_cols:\n",
    "             df_scaled[col] = df_scaled.groupby('patient_id')[col].transform(robust_scale_ignore_nan)\n",
    "\n",
    "        # 3. Fill remaining NaNs with -1 (as requested to indicate missingness)\n",
    "        missing_count = df_scaled[numeric_cols].isna().sum().sum()\n",
    "        logging.info(f\"  Filling {missing_count} remaining NaNs with -1.0\")\n",
    "        df_scaled[numeric_cols] = df_scaled[numeric_cols].fillna(-1.0)\n",
    "\n",
    "        logging.info(\"\u2713 Scaling and Imputation complete.\")\n",
    "\n",
    "    # --- Sequence Creation ---\n",
    "    max_seq_len = 256\n",
    "    X_seq_list = []\n",
    "    y_seq_list = []\n",
    "    patient_ids = []\n",
    "\n",
    "    all_patient_ids = sorted(df_scaled['patient_id'].unique())\n",
    "    half_n = max(1, len(all_patient_ids) // 1)\n",
    "    patient_ids_to_use = all_patient_ids[:half_n]\n",
    "\n",
    "    for patient_id in patient_ids_to_use:\n",
    "        mask = df_scaled['patient_id'] == patient_id\n",
    "        X_pat = df_scaled.loc[mask, numeric_cols].values\n",
    "        y_pat = df_scaled.loc[mask, 'SepsisLabel'].values if 'SepsisLabel' in df_scaled.columns else np.ones(mask.sum())\n",
    "\n",
    "        if len(X_pat) > max_seq_len:\n",
    "            X_pat = X_pat[-max_seq_len:, :]\n",
    "            y_pat = y_pat[-max_seq_len:]\n",
    "        else:\n",
    "            pad_len = max_seq_len - len(X_pat)\n",
    "            X_pat = np.vstack([X_pat, np.zeros((pad_len, X_pat.shape[1]))])\n",
    "            y_pat = np.concatenate([y_pat, np.zeros(pad_len)])\n",
    "\n",
    "        X_seq_list.append(X_pat)\n",
    "        y_seq_list.append(y_pat)\n",
    "        patient_ids.append(patient_id)\n",
    "\n",
    "    X_seq = np.array(X_seq_list)\n",
    "    y_seq = np.array(y_seq_list)\n",
    "\n",
    "    n = len(X_seq)\n",
    "    idx = np.arange(n)\n",
    "    np.random.shuffle(idx)\n",
    "    n_train = int(0.8 * n)\n",
    "    n_val = int(0.1 * n)\n",
    "\n",
    "    X_train_seq = torch.tensor(X_seq[idx[:n_train]], dtype=torch.float32)\n",
    "    y_train_seq = torch.tensor(y_seq[idx[:n_train]], dtype=torch.float32)\n",
    "    X_val_seq = torch.tensor(X_seq[idx[n_train:n_train+n_val]], dtype=torch.float32)\n",
    "    y_val_seq = torch.tensor(y_seq[idx[n_train:n_train+n_val]], dtype=torch.float32)\n",
    "    X_test_seq = torch.tensor(X_seq[idx[n_train+n_val:]], dtype=torch.float32)\n",
    "    y_test_seq = torch.tensor(y_seq[idx[n_train+n_val:]], dtype=torch.float32)\n",
    "\n",
    "    data_bundle = {\n",
    "        'X_train_seq': X_train_seq, 'y_train_seq': y_train_seq,\n",
    "        'X_val_seq': X_val_seq, 'y_val_seq': y_val_seq,\n",
    "        'X_test_seq': X_test_seq, 'y_test_seq': y_test_seq,\n",
    "        'y_seq_list': y_seq_list, 'idx': idx,\n",
    "        'n_train': n_train, 'n_val': n_val\n",
    "    }\n",
    "\n",
    "    if use_cache:\n",
    "        logging.info(f\"Saving preprocessed data to {cache_path}...\")\n",
    "        try:\n",
    "            with open(cache_path, 'wb') as f:\n",
    "                pickle.dump(data_bundle, f)\n",
    "            logging.info(\"\u2713 Data saved to cache.\")\n",
    "        except Exception as e:\n",
    "            logging.info(f\"\u26a0 Failed to save cache: {e}\")\n",
    "\n",
    "    return data_bundle\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architectures\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# --- PyTorch Models ---\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, hp, in_dim):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.RNN(in_dim, hp['units'], batch_first=True)\n",
    "        self.drop = nn.Dropout(hp['dropout'])\n",
    "        self.fc = nn.Linear(hp['units'], 1)\n",
    "        self.final_act = hp.get('final_activation', 'sigmoid')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.rnn(x)\n",
    "        x = self.drop(x)\n",
    "        out = self.fc(x)\n",
    "        if self.final_act == 'sigmoid':\n",
    "            return torch.sigmoid(out)\n",
    "        elif self.final_act == 'tanh':\n",
    "            return torch.tanh(out)\n",
    "        elif self.final_act == 'relu':\n",
    "            return torch.relu(out)\n",
    "        elif self.final_act == 'leakyrelu':\n",
    "            return torch.nn.functional.leaky_relu(out)\n",
    "        elif self.final_act == 'elu':\n",
    "            return torch.nn.functional.elu(out)\n",
    "        return out\n",
    "\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, hp, in_dim):\n",
    "        super().__init__()\n",
    "        ks = hp.get('kernel_size', 3)\n",
    "        st = hp.get('stride', 1)\n",
    "        self.conv1 = nn.Conv1d(in_dim, hp['f1'], ks, stride=st, padding=ks//2)\n",
    "        self.conv2 = nn.Conv1d(hp['f1'], hp['f2'], ks, stride=st, padding=ks//2)\n",
    "        self.drop = nn.Dropout(hp['dropout'])\n",
    "        self.fc = nn.Linear(hp['f2'], 1)\n",
    "        self.final_act = hp.get('final_activation', 'sigmoid')\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (N, L, C) -> (N, C, L)\n",
    "        x = x.transpose(1, 2)\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = self.drop(x)\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = self.drop(x)\n",
    "        # (N, C, L) -> (N, L, C)\n",
    "        x = x.transpose(1, 2)\n",
    "        out = self.fc(x)\n",
    "        if self.final_act == 'sigmoid':\n",
    "            return torch.sigmoid(out)\n",
    "        elif self.final_act == 'tanh':\n",
    "            return torch.tanh(out)\n",
    "        elif self.final_act == 'relu':\n",
    "            return torch.relu(out)\n",
    "        elif self.final_act == 'leakyrelu':\n",
    "            return torch.nn.functional.leaky_relu(out)\n",
    "        elif self.final_act == 'elu':\n",
    "            return torch.nn.functional.elu(out)\n",
    "        return out\n",
    "\n",
    "class LGSTMModel(nn.Module):\n",
    "    def __init__(self, hp, in_dim):\n",
    "        super().__init__()\n",
    "        self.lstm1 = nn.LSTM(in_dim, hp['u1'], batch_first=True)\n",
    "        self.drop1 = nn.Dropout(hp['dropout'])\n",
    "        self.lstm2 = nn.LSTM(hp['u1'], hp['u2'], batch_first=True)\n",
    "        self.drop2 = nn.Dropout(hp['dropout'])\n",
    "        self.fc = nn.Linear(hp['u2'], 1)\n",
    "        self.final_act = hp.get('final_activation', 'sigmoid')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm1(x)\n",
    "        x = self.drop1(x)\n",
    "        x, _ = self.lstm2(x)\n",
    "        x = self.drop2(x)\n",
    "        out = self.fc(x)\n",
    "        if self.final_act == 'sigmoid':\n",
    "            return torch.sigmoid(out)\n",
    "        elif self.final_act == 'tanh':\n",
    "            return torch.tanh(out)\n",
    "        elif self.final_act == 'relu':\n",
    "            return torch.relu(out)\n",
    "        elif self.final_act == 'leakyrelu':\n",
    "            return torch.nn.functional.leaky_relu(out)\n",
    "        elif self.final_act == 'elu':\n",
    "            return torch.nn.functional.elu(out)\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# --- Training & Evaluation Functions ---\n",
    "\n",
    "def get_loss_fn(loss_name):\n",
    "    if loss_name == 'binary_crossentropy':\n",
    "        return nn.BCELoss()\n",
    "    elif loss_name == 'mse':\n",
    "        return nn.MSELoss()\n",
    "    elif loss_name == 'hinge':\n",
    "        return lambda y_pred, y_true: torch.mean(torch.clamp(1 - y_true * y_pred, min=0))\n",
    "    return nn.BCELoss()\n",
    "\n",
    "def predict_batched(model, X, batch_size=32):\n",
    "    \"\"\"\n",
    "    Perform inference in batches to avoid OOM.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    preds_list = []\n",
    "    dataset = TensorDataset(X)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for xb, in dataloader:\n",
    "            xb = xb.to(device)\n",
    "            out = model(xb)\n",
    "            preds_list.append(out.cpu())\n",
    "\n",
    "    return torch.cat(preds_list, dim=0)\n",
    "\n",
    "def map_output_to_prob(preds, final_act):\n",
    "    if final_act == 'tanh':\n",
    "        return (preds + 1.0) / 2.0\n",
    "    if final_act == 'relu' or final_act == 'leakyrelu' or final_act == 'elu':\n",
    "        return np.clip(preds, 0.0, 1.0)\n",
    "    return preds\n",
    "\n",
    "# Score Utility Function\n",
    "def compute_prediction_utility(labels, predictions, dt_early=-12, dt_optimal=-6, dt_late=3.0,\n",
    "                             max_u_tp=1, min_u_fn=-2, u_fp=-0.05, u_tn=0):\n",
    "    n_patients = len(labels)\n",
    "    observed_utility = 0.0\n",
    "    best_utility = 0.0\n",
    "    inactive_utility = 0.0\n",
    "\n",
    "    for i in range(n_patients):\n",
    "        label = labels[i]\n",
    "        pred = predictions[i]\n",
    "        n = len(label)\n",
    "\n",
    "        if np.any(label):\n",
    "            t_sepsis = np.argmax(label == 1)\n",
    "            is_septic = True\n",
    "        else:\n",
    "            t_sepsis = float('inf')\n",
    "            is_septic = False\n",
    "\n",
    "        u_vector = np.zeros(n)\n",
    "        if is_septic:\n",
    "            t_diff = np.arange(n) - t_sepsis\n",
    "            mask_slope = (t_diff >= dt_early) & (t_diff <= dt_optimal)\n",
    "            u_vector[mask_slope] = max_u_tp * (t_diff[mask_slope] - dt_early) / (dt_optimal - dt_early)\n",
    "            mask_plateau = (t_diff > dt_optimal) & (t_diff <= dt_late)\n",
    "            u_vector[mask_plateau] = max_u_tp\n",
    "\n",
    "        term1 = pred * u_vector\n",
    "        term2 = (pred == 1) & (u_vector == 0)\n",
    "        p_utility = np.sum(term1) + np.sum(term2.astype(float) * u_fp)\n",
    "        observed_utility += p_utility\n",
    "\n",
    "        best_pred = (u_vector > 0).astype(int)\n",
    "        term1_best = best_pred * u_vector\n",
    "        term2_best = (best_pred == 1) & (u_vector == 0)\n",
    "        b_utility = np.sum(term1_best) + np.sum(term2_best.astype(float) * u_fp)\n",
    "        best_utility += b_utility\n",
    "\n",
    "    if best_utility == inactive_utility:\n",
    "        normalized_utility = 0.0\n",
    "    else:\n",
    "        normalized_utility = (observed_utility - inactive_utility) / (best_utility - inactive_utility)\n",
    "\n",
    "    return normalized_utility, observed_utility\n",
    "\n",
    "def evaluate_utility_val(model, X_val, val_indices, y_seq_list, max_seq_len, final_act='sigmoid'):\n",
    "    model.eval()\n",
    "    probs_tensor = predict_batched(model, X_val, batch_size=32)\n",
    "    probs_flat = probs_tensor.numpy().flatten()\n",
    "    # Map to prob\n",
    "    probs_mapped = map_output_to_prob(probs_flat, final_act).reshape(probs_tensor.shape[:2])\n",
    "\n",
    "    # Threshold 0.5\n",
    "    preds_padded = (probs_mapped > 0.5).astype(int)\n",
    "\n",
    "    labels_list = []\n",
    "    preds_list = []\n",
    "\n",
    "    for k, idx_val in enumerate(val_indices):\n",
    "        true_seq = y_seq_list[idx_val]\n",
    "        pat_len = min(len(true_seq), max_seq_len)\n",
    "\n",
    "        p_seq = preds_padded[k, :pat_len]\n",
    "        labels_list.append(true_seq[:pat_len])\n",
    "        preds_list.append(p_seq)\n",
    "\n",
    "    norm, raw = compute_prediction_utility(labels_list, preds_list)\n",
    "    return norm\n",
    "\n",
    "def train_model_pyt(model_class, hp, in_dim, X_train, y_train, X_val, y_val, epochs):\n",
    "    model = model_class(hp, in_dim).to(device)\n",
    "    lr = hp.get('lr', 1e-3)\n",
    "    opt_name = hp.get('optimizer', 'adam').lower()\n",
    "\n",
    "    if opt_name == 'adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    elif opt_name == 'adamw':\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "    elif opt_name == 'sgd':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "    elif opt_name == 'rmsprop':\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=lr)\n",
    "    else:\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    loss_name = hp.get('loss', 'binary_crossentropy')\n",
    "    criterion = get_loss_fn(loss_name)\n",
    "\n",
    "    if loss_name == 'hinge':\n",
    "        y_train_target = (y_train.clone() * 2.0) - 1.0\n",
    "    else:\n",
    "        y_train_target = y_train\n",
    "\n",
    "    num_workers = 0\n",
    "    use_pin_memory = (device.type == 'cuda')\n",
    "\n",
    "    ds_train = TensorDataset(X_train, y_train_target)\n",
    "    dl_train = DataLoader(ds_train, batch_size=hp['batch_size'], shuffle=True,\n",
    "                          num_workers=num_workers, pin_memory=use_pin_memory)\n",
    "\n",
    "    history = {'loss': []}\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for xb, yb in dl_train:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(xb)\n",
    "            loss = criterion(out, yb.unsqueeze(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        history['loss'].append(train_loss / len(dl_train))\n",
    "\n",
    "    return model, history\n",
    "\n",
    "def grid_search_pyt(model_class, grid, name, in_dim, X_train, y_train, X_val, y_val,\n",
    "                   val_indices, y_seq_list, max_seq_len, epochs=10):\n",
    "    results = []\n",
    "\n",
    "    os.makedirs('/tmp/icu_tune', exist_ok=True)\n",
    "\n",
    "    for hp in ParameterGrid(grid):\n",
    "        if hp['loss'] == 'binary_crossentropy' and hp['final_activation'] != 'sigmoid': continue\n",
    "        if hp['loss'] == 'hinge' and hp['final_activation'] == 'sigmoid': continue\n",
    "\n",
    "        logging.info(f'[{name}] trying {hp}')\n",
    "        model = None\n",
    "        try:\n",
    "            model, hist = train_model_pyt(model_class, hp, in_dim, X_train, y_train, X_val, y_val, epochs)\n",
    "            # Evaluate Utility\n",
    "            util_score = evaluate_utility_val(model, X_val, val_indices, y_seq_list, max_seq_len, hp.get('final_activation', 'sigmoid'))\n",
    "\n",
    "            # Evaluate AUC on Val\n",
    "            model.eval()\n",
    "            val_out = predict_batched(model, X_val, batch_size=32)\n",
    "            val_preds_np = val_out.numpy().flatten()\n",
    "            val_preds_prob = map_output_to_prob(val_preds_np, hp.get('final_activation', 'sigmoid'))\n",
    "            try:\n",
    "                auc_score = roc_auc_score(y_val.numpy().flatten(), val_preds_prob)\n",
    "            except:\n",
    "                    auc_score = 0.5\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.info(f'  fit failed: {e}')\n",
    "            util_score = -999.0\n",
    "            auc_score = 0.5\n",
    "\n",
    "        logging.info(f'  Utility={util_score:.4f}, AUC={auc_score:.4f}')\n",
    "        results.append({**hp, 'utility': util_score, 'auc': auc_score})\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "    return df\n",
    "\n",
    "def evaluate_sequence_model(model, X_seq, y_seq, threshold=0.5, name='model', final_act='sigmoid'):\n",
    "    model.eval()\n",
    "    preds = predict_batched(model, X_seq, batch_size=32)\n",
    "\n",
    "    preds_flat = preds.numpy().flatten()\n",
    "    preds_prob = map_output_to_prob(preds_flat, final_act)\n",
    "    y_flat = y_seq.numpy().flatten()\n",
    "\n",
    "    auc = roc_auc_score(y_flat, preds_prob)\n",
    "    avg_prec = average_precision_score(y_flat, preds_prob)\n",
    "    y_pred_bin = (preds_prob >= threshold).astype(int)\n",
    "    prec = precision_score(y_flat, y_pred_bin, zero_division=0)\n",
    "    rec = recall_score(y_flat, y_pred_bin, zero_division=0)\n",
    "    f1 = f1_score(y_flat, y_pred_bin, zero_division=0)\n",
    "    cm = confusion_matrix(y_flat, y_pred_bin)\n",
    "\n",
    "    return {\n",
    "        'auc': auc, 'ap': avg_prec, 'precision': prec, 'recall': rec, 'f1': f1,\n",
    "        'confusion_matrix': cm, 'y_true': y_flat, 'y_pred': preds_prob, 'name': name\n",
    "    }\n",
    "\n",
    "def plot_patient_prediction(model, X_seq, y_seq_list, original_indices, max_seq_len, final_act='sigmoid', prefix=''):\n",
    "    \"\"\"\n",
    "    Plots the prediction score for each hour for a few sample patients.\n",
    "    \"\"\"\n",
    "    os.makedirs(plot_dir, exist_ok=True)\n",
    "    model.eval()\n",
    "    preds = predict_batched(model, X_seq, batch_size=32)\n",
    "\n",
    "    preds_np = preds.numpy()\n",
    "\n",
    "    # Pick 2 random patients (one positive, one negative if possible)\n",
    "    np.random.seed(99) # Fixed seed for reproducibility\n",
    "    sample_indices = np.random.choice(len(original_indices), size=min(3, len(original_indices)), replace=False)\n",
    "\n",
    "    for i, idx_in_batch in enumerate(sample_indices):\n",
    "        true_pat_idx = original_indices[idx_in_batch]\n",
    "        true_seq = y_seq_list[true_pat_idx]\n",
    "        pat_len = min(len(true_seq), max_seq_len)\n",
    "\n",
    "        pred_seq_raw = preds_np[idx_in_batch, :pat_len].flatten()\n",
    "        pred_seq_prob = map_output_to_prob(pred_seq_raw, final_act)\n",
    "\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.plot(range(pat_len), true_seq[:pat_len], label='True Label', color='black', linestyle='--')\n",
    "        plt.plot(range(pat_len), pred_seq_prob, label='Prediction Score', color='blue', alpha=0.7)\n",
    "        plt.axhline(0.5, color='red', linestyle=':', label='Threshold')\n",
    "        plt.ylim(-0.1, 1.1)\n",
    "        plt.xlabel('Hours (Time Steps)')\n",
    "        plt.ylabel('Sepsis Probability / Label')\n",
    "        plt.title(f'Patient {true_pat_idx} - Hourly Predictions ({prefix})')\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{plot_dir}/{prefix}_patient_{true_pat_idx}_hourly.png')\n",
    "        plt.close()\n",
    "\n",
    "def plot_all_results(eval_metrics_list, model_dfs, final_epoch_results, best_model_name):\n",
    "    # Ensure plots directory exists\n",
    "    os.makedirs(plot_dir, exist_ok=True)\n",
    "\n",
    "    # 1. Detailed Hyperparam plots for ALL models\n",
    "    for name, df in model_dfs.items():\n",
    "        if df is not None and not df.empty:\n",
    "            # Identify params (exclude metrics and identifiers)\n",
    "            exclude = ['utility', 'auc', 'score']\n",
    "            params = [c for c in df.columns if c not in exclude and df[c].nunique() > 1]\n",
    "\n",
    "            if not params:\n",
    "                continue\n",
    "\n",
    "            # Layout: Grid of subplots\n",
    "            ncols = 3\n",
    "            nrows = (len(params) + ncols - 1) // ncols\n",
    "            fig, axs = plt.subplots(nrows, ncols, figsize=(6 * ncols, 5 * nrows))\n",
    "\n",
    "            # Ensure axs is iterable even if only 1 subplot\n",
    "            if nrows * ncols == 1:\n",
    "                axs = np.array([axs])\n",
    "            axs = axs.ravel()\n",
    "\n",
    "            for i, param in enumerate(params):\n",
    "                # Get max score per param value\n",
    "                best_scores = df.groupby(param)['utility'].max()\n",
    "\n",
    "                # Plot\n",
    "                x_vals = best_scores.index.astype(str)\n",
    "                y_vals = best_scores.values\n",
    "\n",
    "                axs[i].bar(x_vals, y_vals, color='skyblue', edgecolor='black')\n",
    "                axs[i].set_title(f'Effect of {param}')\n",
    "                axs[i].set_ylabel('Best Utility Score')\n",
    "                axs[i].tick_params(axis='x', rotation=45)\n",
    "                axs[i].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "            # Turn off unused subplots\n",
    "            for j in range(i + 1, len(axs)):\n",
    "                axs[j].axis('off')\n",
    "\n",
    "            plt.suptitle(f'{name} Hyperparameter Impact (Utility)', fontsize=16)\n",
    "            plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "            plt.savefig(f'{plot_dir}/{name}_hyperparams.png')\n",
    "            plt.close()\n",
    "\n",
    "    # 2. Final Epochs Performance Line Plot\n",
    "    if final_epoch_results:\n",
    "        epochs = [r['epoch'] for r in final_epoch_results]\n",
    "        utils = [r['utility'] for r in final_epoch_results]\n",
    "        aucs = [r['auc'] for r in final_epoch_results]\n",
    "\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.plot(epochs, utils, 'o-', label='Utility Score', color='green')\n",
    "        plt.plot(epochs, aucs, 's-', label='AUC', color='blue')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Score')\n",
    "        plt.title(f'{best_model_name} Scores vs Epochs')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{plot_dir}/{best_model_name}_epochs_trend.png')\n",
    "        plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def run_training_pipeline(data_bundle, result_prefix=\"\"):\n",
    "    \"\"\"\n",
    "    Executes the training pipeline (Grid Search -> Candidate Selection -> Final Training)\n",
    "    for a given dataset bundle.\n",
    "    \"\"\"\n",
    "    X_train_seq = data_bundle['X_train_seq']\n",
    "    y_train_seq = data_bundle['y_train_seq']\n",
    "    X_val_seq = data_bundle['X_val_seq']\n",
    "    y_val_seq = data_bundle['y_val_seq']\n",
    "    X_test_seq = data_bundle['X_test_seq']\n",
    "    y_test_seq = data_bundle['y_test_seq']\n",
    "    y_seq_list = data_bundle['y_seq_list']\n",
    "    idx = data_bundle['idx']\n",
    "    n_train = data_bundle['n_train']\n",
    "    n_val = data_bundle['n_val']\n",
    "\n",
    "    max_seq_len = X_train_seq.shape[1]\n",
    "    in_dim = X_train_seq.shape[2]\n",
    "\n",
    "    logging.info(f\"[{result_prefix}] Train: {X_train_seq.shape}, Val: {X_val_seq.shape}, Test: {X_test_seq.shape}\")\n",
    "\n",
    "    candidate_configs = []\n",
    "\n",
    "    if args.use_last_final:\n",
    "        logging.info(\"=== USING LAST FINAL CONFIGURATIONS (SKIPPING GRID SEARCH) ===\")\n",
    "        configs = get_last_final_configs()\n",
    "        for c in configs:\n",
    "            if c['model_type'] == 'RNN':\n",
    "                model_cls = RNNModel\n",
    "            elif c['model_type'] == 'CNN':\n",
    "                model_cls = CNNModel\n",
    "            elif c['model_type'] == 'LGSTM':\n",
    "                model_cls = LGSTMModel\n",
    "            else:\n",
    "                logging.warning(f\"Unknown model type: {c['model_type']}\")\n",
    "                continue\n",
    "\n",
    "            candidate_configs.append({\n",
    "                'name': c['name'],\n",
    "                'model_cls': model_cls,\n",
    "                'hp': c['hp'],\n",
    "                'score_type': c['score_type'],\n",
    "                'val_score': c['val_score']\n",
    "            })\n",
    "\n",
    "    else:\n",
    "        logging.info(f\"=== [{result_prefix}] FULL GRID SEARCH ON ALL MODELS (PYTORCH) - OFFICIAL SCORE & AUC ===\")\n",
    "\n",
    "        big_rnn_grid = {\n",
    "            'units': [64, 128],\n",
    "            'dropout': [0.2, 0.4],\n",
    "            'lr': [1e-3, 5e-4],\n",
    "            'batch_size': [32, 64],\n",
    "            #'batch_size': [32],\n",
    "            'final_activation': ['sigmoid', 'leakyrelu', 'elu', 'tanh'],\n",
    "            #'final_activation': ['sigmoid', 'elu' ],\n",
    "            'loss': ['binary_crossentropy', 'mse', 'hinge'],\n",
    "            #'loss': ['binary_crossentropy', 'mse'],\n",
    "            'optimizer': ['adam', 'adamw']\n",
    "            #'optimizer': ['adam']\n",
    "        }\n",
    "\n",
    "        big_cnn_grid = {\n",
    "            'f1': [32, 64],\n",
    "            #'f2': [64],\n",
    "            'f2': [64, 128],\n",
    "            'kernel_size': [3, 5],\n",
    "            #'kernel_size': [3],\n",
    "            'stride': [1, 2],\n",
    "            #'stride': [1],\n",
    "            'dropout': [0.2, 0.4],\n",
    "            'lr': [1e-3, 5e-4],\n",
    "            #'batch_size': [32],\n",
    "            'batch_size': [32, 64],\n",
    "            'final_activation': ['sigmoid', 'leakyrelu', 'elu', 'tanh'],\n",
    "            #'final_activation': ['sigmoid','elu'],\n",
    "            'loss': ['binary_crossentropy', 'mse', 'hinge'],\n",
    "            #'loss': ['binary_crossentropy', 'mse'],\n",
    "            'optimizer': ['adam', 'adamw']\n",
    "            #'optimizer': ['adam']\n",
    "        }\n",
    "\n",
    "        big_lgstm_grid = {\n",
    "                'u1': [64, 128],\n",
    "            #'u1': [64],\n",
    "            'u2': [32, 64],\n",
    "            'dropout': [0.2, 0.4],\n",
    "            'lr': [1e-3, 5e-4],\n",
    "            'batch_size': [8, 16], # Updated as requested\n",
    "            #'batch_size': [16], # Updated as requested\n",
    "            'final_activation': ['sigmoid', 'leakyrelu', 'elu', 'tanh'],\n",
    "            #'final_activation': ['sigmoid','elu'],\n",
    "            'loss': ['binary_crossentropy', 'mse', 'hinge'],\n",
    "            #'loss': ['binary_crossentropy', 'mse'],\n",
    "            'optimizer': ['adam', 'adamw']\n",
    "            #'optimizer': ['adam']\n",
    "        }\n",
    "\n",
    "        # Indices for Validation Eval\n",
    "        val_indices = idx[n_train:n_train+n_val]\n",
    "\n",
    "        # Run Grid Search\n",
    "        logging.info(f\"--- Tuning RNN ({result_prefix}) ---\")\n",
    "        rnn_df = grid_search_pyt(RNNModel, big_rnn_grid, f'{result_prefix}_RNN', in_dim,\n",
    "                                X_train_seq, y_train_seq, X_val_seq, y_val_seq,\n",
    "                                val_indices, y_seq_list, max_seq_len, epochs=5)\n",
    "\n",
    "        logging.info(f\"\\n--- Tuning CNN ({result_prefix}) ---\")\n",
    "        cnn_df = grid_search_pyt(CNNModel, big_cnn_grid, f'{result_prefix}_CNN', in_dim,\n",
    "                                X_train_seq, y_train_seq, X_val_seq, y_val_seq,\n",
    "                                val_indices, y_seq_list, max_seq_len, epochs=5)\n",
    "\n",
    "        logging.info(f\"\\n--- Tuning LGSTM ({result_prefix}) ---\")\n",
    "        lgstm_df = grid_search_pyt(LGSTMModel, big_lgstm_grid, f'{result_prefix}_LGSTM', in_dim,\n",
    "                                X_train_seq, y_train_seq, X_val_seq, y_val_seq,\n",
    "                                val_indices, y_seq_list, max_seq_len, epochs=5)\n",
    "\n",
    "        # Plot Hyperparameters\n",
    "        logging.info(\"Generating Hyperparameter Plots...\")\n",
    "        model_dfs = {\n",
    "            f'{result_prefix}_RNN': rnn_df,\n",
    "            f'{result_prefix}_CNN': cnn_df,\n",
    "            f'{result_prefix}_LGSTM': lgstm_df\n",
    "        }\n",
    "        plot_all_results([], model_dfs, [], \"\")\n",
    "\n",
    "        # --- Select Top 2 by Utility and Top 2 by AUC for EACH model ---\n",
    "\n",
    "        for name, df, model_cls in [('RNN', rnn_df, RNNModel), ('CNN', cnn_df, CNNModel), ('LGSTM', lgstm_df, LGSTMModel)]:\n",
    "            if df.empty: continue\n",
    "\n",
    "            # Top 2 by Utility\n",
    "            top_util = df.sort_values('utility', ascending=False).head(2)\n",
    "            for _, row in top_util.iterrows():\n",
    "                hp = row.drop(['utility', 'auc']).to_dict()\n",
    "                candidate_configs.append({\n",
    "                    'name': f'{result_prefix}_{name}_BestUtil_{_}',\n",
    "                    'model_cls': model_cls,\n",
    "                    'model_type': name,\n",
    "                    'hp': hp,\n",
    "                    'score_type': 'utility',\n",
    "                    'val_score': row['utility']\n",
    "                })\n",
    "\n",
    "            # Top 2 by AUC\n",
    "            top_auc = df.sort_values('auc', ascending=False).head(2)\n",
    "            for _, row in top_auc.iterrows():\n",
    "                hp = row.drop(['utility', 'auc']).to_dict()\n",
    "                is_dup = False\n",
    "                for c in candidate_configs:\n",
    "                    if c['hp'] == hp and c['name'].startswith(f'{result_prefix}_{name}'):\n",
    "                        is_dup = True\n",
    "                        break\n",
    "                if not is_dup:\n",
    "                    candidate_configs.append({\n",
    "                        'name': f'{result_prefix}_{name}_BestAUC_{_}',\n",
    "                        'model_cls': model_cls,\n",
    "                        'model_type': name,\n",
    "                        'hp': hp,\n",
    "                        'score_type': 'auc',\n",
    "                        'val_score': row['auc']\n",
    "                    })\n",
    "\n",
    "    logging.info(f\"\\n=== [{result_prefix}] Selected {len(candidate_configs)} Candidate Configurations for Final Training ===\")\n",
    "    \n",
    "    # Save candidate configs to JSON for future use\n",
    "    json_path = \"best_model_configs.json\"\n",
    "    # Load existing if any (to append/merge if running multiple prefixes)\n",
    "    existing_configs = []\n",
    "    if os.path.exists(json_path):\n",
    "        try:\n",
    "            with open(json_path, 'r') as f:\n",
    "                existing_configs = json.load(f)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Simple merge: add new ones, replacing if name exists (though names should be unique per run usually)\n",
    "    # Actually, let's just append and filter duplicates by name later if needed, \n",
    "    # but since names include prefix, they should be unique.\n",
    "    \n",
    "    # We can't save 'model_cls' class object to JSON, so we remove it for saving\n",
    "    configs_to_save = []\n",
    "    for c in candidate_configs:\n",
    "        c_save = c.copy()\n",
    "        if 'model_cls' in c_save:\n",
    "            del c_save['model_cls']\n",
    "        configs_to_save.append(c_save)\n",
    "    \n",
    "    # Update existing list\n",
    "    # Remove old entries with same names as new ones to avoid dups\n",
    "    new_names = {c['name'] for c in configs_to_save}\n",
    "    existing_configs = [c for c in existing_configs if c['name'] not in new_names]\n",
    "    final_save_list = existing_configs + configs_to_save\n",
    "    \n",
    "    try:\n",
    "        with open(json_path, 'w') as f:\n",
    "            json.dump(final_save_list, f, indent=4)\n",
    "        logging.info(f\"\u2713 Saved {len(final_save_list)} configurations to {json_path}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to save configs to JSON: {e}\")\n",
    "\n",
    "    for c in candidate_configs:\n",
    "        logging.info(f\"  {c['name']} (Val {c['score_type'].title()}: {c['val_score']:.4f}) - HP: {c['hp']}\")\n",
    "\n",
    "    # --- Final Training of Candidates ---\n",
    "    test_indices = idx[n_train+n_val:]\n",
    "    final_results = []\n",
    "\n",
    "    all_candidates_epoch_trends = {}\n",
    "    final_epochs_list = [10, 25, 50, 100, 150]\n",
    "\n",
    "    for config in candidate_configs:\n",
    "        name = config['name']\n",
    "        model_cls = config['model_cls']\n",
    "        hp = config['hp']\n",
    "\n",
    "        logging.info(f\"\\nTraining {name} (Epoch Analysis)...\")\n",
    "\n",
    "        candidate_trend = []\n",
    "        model = None\n",
    "\n",
    "        for n_ep in final_epochs_list:\n",
    "            logging.info(f\"  > Epochs: {n_ep}\")\n",
    "            # Train Fresh Model\n",
    "            model, hist = train_model_pyt(model_cls, hp, in_dim, X_train_seq, y_train_seq, X_val_seq, y_val_seq, epochs=n_ep)\n",
    "\n",
    "            # Evaluate\n",
    "            metrics = evaluate_sequence_model(model, X_test_seq, y_test_seq, threshold=0.5, name=name, final_act=hp.get('final_activation', 'sigmoid'))\n",
    "            util_score = evaluate_utility_val(model, X_test_seq, test_indices, y_seq_list, max_seq_len, hp.get('final_activation', 'sigmoid'))\n",
    "\n",
    "            logging.info(f\"    -> Ep {n_ep}: Util={util_score:.4f}, AUC={metrics['auc']:.4f}\")\n",
    "\n",
    "            candidate_trend.append({\n",
    "                'epoch': n_ep,\n",
    "                'utility': util_score,\n",
    "                'auc': metrics['auc'],\n",
    "                'f1': metrics['f1']\n",
    "            })\n",
    "\n",
    "        all_candidates_epoch_trends[name] = candidate_trend\n",
    "\n",
    "        # Use the result from the max epochs (150) for the final summary\n",
    "        last_res = candidate_trend[-1]\n",
    "        \n",
    "        # Flatten HP for CSV\n",
    "        res_entry = {\n",
    "            'name': name,\n",
    "            'dataset_method': result_prefix,\n",
    "            'utility': last_res['utility'],\n",
    "            'auc': last_res['auc'],\n",
    "            'f1': last_res['f1']\n",
    "        }\n",
    "        # Add hyperparams\n",
    "        for k, v in hp.items():\n",
    "            res_entry[k] = v\n",
    "            \n",
    "        final_results.append(res_entry)\n",
    "\n",
    "        # Plot Patient Predictions (using the model from the last iteration)\n",
    "        plot_patient_prediction(model, X_test_seq, y_seq_list, test_indices, max_seq_len, hp.get('final_activation', 'sigmoid'), prefix=name)\n",
    "\n",
    "    # --- Plot Combined Trends ---\n",
    "    os.makedirs(plot_dir, exist_ok=True)\n",
    "\n",
    "    # Utility Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for name, trend in all_candidates_epoch_trends.items():\n",
    "        eps = [t['epoch'] for t in trend]\n",
    "        uts = [t['utility'] for t in trend]\n",
    "        plt.plot(eps, uts, 'o-', label=name)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Utility Score')\n",
    "    plt.title(f'Utility Score vs Epochs ({result_prefix})')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{plot_dir}/{result_prefix}_All_Candidates_Epochs_Utility.png')\n",
    "    plt.close()\n",
    "\n",
    "    # AUC Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for name, trend in all_candidates_epoch_trends.items():\n",
    "        eps = [t['epoch'] for t in trend]\n",
    "        aucs = [t['auc'] for t in trend]\n",
    "        plt.plot(eps, aucs, 's-', label=name)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('AUC Score')\n",
    "    plt.title(f'AUC Score vs Epochs ({result_prefix})')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{plot_dir}/{result_prefix}_All_Candidates_Epochs_AUC.png')\n",
    "    plt.close()\n",
    "\n",
    "    return final_results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def plot_preprocessing_comparison(df_results):\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    Plots a comparison of best utility and AUC scores for each preprocessing method.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if df_results.empty or 'dataset_method' not in df_results.columns:\n",
    "\n",
    "        return\n",
    "\n",
    "\n",
    "\n",
    "    os.makedirs(plot_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "    # Filter to get the best model per preprocessing method\n",
    "\n",
    "    best_per_method = df_results.loc[df_results.groupby('dataset_method')['utility'].idxmax()]\n",
    "\n",
    "\n",
    "\n",
    "    methods = best_per_method['dataset_method'].unique()\n",
    "\n",
    "\n",
    "\n",
    "    fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "\n",
    "\n",
    "    x = np.arange(len(methods))\n",
    "\n",
    "    width = 0.35\n",
    "\n",
    "\n",
    "\n",
    "    # Utility Bars\n",
    "\n",
    "    rects1 = ax1.bar(x - width/2, best_per_method['utility'], width, label='Best Utility', color='skyblue', edgecolor='black')\n",
    "\n",
    "    ax1.set_xlabel('Preprocessing Method')\n",
    "\n",
    "    ax1.set_ylabel('Utility Score', color='blue')\n",
    "\n",
    "    ax1.tick_params(axis='y', labelcolor='blue')\n",
    "\n",
    "    ax1.set_xticks(x)\n",
    "\n",
    "    ax1.set_xticklabels(methods)\n",
    "\n",
    "\n",
    "\n",
    "    # AUC Bars (secondary axis)\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "\n",
    "    rects2 = ax2.bar(x + width/2, best_per_method['auc'], width, label='Best AUC', color='lightgreen', edgecolor='black')\n",
    "\n",
    "    ax2.set_ylabel('AUC Score', color='green')\n",
    "\n",
    "    ax2.tick_params(axis='y', labelcolor='green')\n",
    "\n",
    "    ax2.set_ylim(0.5, 1.0) # AUC usually > 0.5\n",
    "\n",
    "\n",
    "\n",
    "    plt.title('Best Performance by Preprocessing Method')\n",
    "\n",
    "\n",
    "\n",
    "    # Combined Legend\n",
    "\n",
    "    lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "\n",
    "    lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "\n",
    "    ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper left')\n",
    "\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.savefig(f'{plot_dir}/preprocessing_comparison.png')\n",
    "\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\n",
    "# --- Execution ---\n",
    "all_final_results = []\n",
    "\n",
    "if args.use_all_pp:\n",
    "    logging.info(\"=== RUNNING TRAINING ON ALL PREPROCESSING METHODS ===\")\n",
    "    methods = ['neg1', 'mean', 'median', 'linear']\n",
    "\n",
    "    for method in methods:\n",
    "        pkl_file = f\"preprocessed_{method}.pkl\"\n",
    "        if not os.path.exists(pkl_file):\n",
    "            logging.warning(f\"File {pkl_file} not found. Skipping...\")\n",
    "            continue\n",
    "\n",
    "        logging.info(f\"\n",
    "\n",
    ">>> PROCESSING DATASET: {method} <<<\")\n",
    "        data = get_data(use_cache=True, cache_path=pkl_file)\n",
    "        results = run_training_pipeline(data, result_prefix=method)\n",
    "        all_final_results.extend(results)\n",
    "\n",
    "else:\n",
    "    # Default single-run behavior\n",
    "    logging.info(\"=== RUNNING SINGLE DATASET TRAINING ===\")\n",
    "    data = get_data(use_cache=args.use_cache)\n",
    "    results = run_training_pipeline(data, result_prefix=\"Default\")\n",
    "    all_final_results.extend(results)\n",
    "\n",
    "# Summary\n",
    "if all_final_results:\n",
    "    res_df = pd.DataFrame(all_final_results).sort_values('utility', ascending=False)\n",
    "    logging.info(\"\n",
    "=== FINAL TEST RESULTS SUMMARY (ALL METHODS) ===\")\n",
    "    logging.info(res_df.to_string(index=False))\n",
    "    \n",
    "    # Generate Preprocessing Comparison Plot\n",
    "    try:\n",
    "        plot_preprocessing_comparison(res_df)\n",
    "        plt.show() # Ensure it shows in notebook\n",
    "    except Exception as e:\n",
    "        print(f\"Plotting failed: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}