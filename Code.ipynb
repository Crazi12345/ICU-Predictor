{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ICU-Predictor (cleaned)\n",
        "\n",
        "This notebook has been cleaned: filler, duplicate, and non-working cells were removed.\n",
        "Sections kept: concise data-download attempt, preprocessing, sequence creation, and model training examples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset\n",
        "This notebook attempts to download the `salikhussaini49/prediction-of-sepsis` dataset using `kagglehub`. If automatic download fails, set `dataset_path` to a local path containing `Dataset.csv`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset downloaded to: /home/tired_atlas/.cache/kagglehub/datasets/salikhussaini49/prediction-of-sepsis/versions/2\n",
            "✓ Found dataset at: /home/tired_atlas/.cache/kagglehub/datasets/salikhussaini49/prediction-of-sepsis/versions/2/Dataset.csv\n",
            "✓ Loaded Dataset: 1552210 rows, 44 columns\n",
            "Columns: ['Unnamed: 0', 'Hour', 'HR', 'O2Sat', 'Temp', 'SBP', 'MAP', 'DBP', 'Resp', 'EtCO2']...\n",
            "First 3 rows:\n",
            "   Unnamed: 0  Hour    HR  O2Sat  Temp  SBP   MAP  DBP  Resp  EtCO2  ...  \\\n",
            "0           0     0   NaN    NaN   NaN  NaN   NaN  NaN   NaN    NaN  ...   \n",
            "1           1     1  65.0  100.0   NaN  NaN  72.0  NaN  16.5    NaN  ...   \n",
            "2           2     2  78.0  100.0   NaN  NaN  42.5  NaN   NaN    NaN  ...   \n",
            "\n",
            "   Fibrinogen  Platelets    Age  Gender  Unit1  Unit2  HospAdmTime  ICULOS  \\\n",
            "0         NaN        NaN  68.54       0    NaN    NaN        -0.02       1   \n",
            "1         NaN        NaN  68.54       0    NaN    NaN        -0.02       2   \n",
            "2         NaN        NaN  68.54       0    NaN    NaN        -0.02       3   \n",
            "\n",
            "   SepsisLabel  Patient_ID  \n",
            "0            0       17072  \n",
            "1            0       17072  \n",
            "2            0       17072  \n",
            "\n",
            "[3 rows x 44 columns]\n",
            "✓ Loaded Dataset: 1552210 rows, 44 columns\n",
            "Columns: ['Unnamed: 0', 'Hour', 'HR', 'O2Sat', 'Temp', 'SBP', 'MAP', 'DBP', 'Resp', 'EtCO2']...\n",
            "First 3 rows:\n",
            "   Unnamed: 0  Hour    HR  O2Sat  Temp  SBP   MAP  DBP  Resp  EtCO2  ...  \\\n",
            "0           0     0   NaN    NaN   NaN  NaN   NaN  NaN   NaN    NaN  ...   \n",
            "1           1     1  65.0  100.0   NaN  NaN  72.0  NaN  16.5    NaN  ...   \n",
            "2           2     2  78.0  100.0   NaN  NaN  42.5  NaN   NaN    NaN  ...   \n",
            "\n",
            "   Fibrinogen  Platelets    Age  Gender  Unit1  Unit2  HospAdmTime  ICULOS  \\\n",
            "0         NaN        NaN  68.54       0    NaN    NaN        -0.02       1   \n",
            "1         NaN        NaN  68.54       0    NaN    NaN        -0.02       2   \n",
            "2         NaN        NaN  68.54       0    NaN    NaN        -0.02       3   \n",
            "\n",
            "   SepsisLabel  Patient_ID  \n",
            "0            0       17072  \n",
            "1            0       17072  \n",
            "2            0       17072  \n",
            "\n",
            "[3 rows x 44 columns]\n"
          ]
        }
      ],
      "source": [
        "# Setup: imports and kagglehub download\n",
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import (roc_auc_score, average_precision_score, roc_curve, \n",
        "                             precision_recall_curve, confusion_matrix, \n",
        "                             precision_score, recall_score, f1_score)\n",
        "\n",
        "# Reproducibility\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Try to download dataset\n",
        "try:\n",
        "    import kagglehub\n",
        "    path = kagglehub.dataset_download(\"salikhussaini49/prediction-of-sepsis\")\n",
        "    print(f\"Dataset downloaded to: {path}\")\n",
        "    dataset_path = path\n",
        "except Exception as e:\n",
        "    print(f\"Download failed ({e}). Using local path or expecting Dataset.csv in working dir.\")\n",
        "    dataset_path = os.getcwd()\n",
        "\n",
        "# Try multiple possible paths for Dataset.csv\n",
        "possible_paths = [\n",
        "    os.path.join(dataset_path, \"Dataset.csv\"),\n",
        "    os.path.join(dataset_path, \"dataset.csv\"),\n",
        "    os.path.join(os.path.expanduser(\"~\"), \"Downloads\", \"Dataset.csv\"),\n",
        "    os.path.join(os.path.expanduser(\"~\"), \"Downloads\", \"dataset.csv\"),\n",
        "    \"Dataset.csv\",\n",
        "    \"dataset.csv\",\n",
        "]\n",
        "\n",
        "csv_file = None\n",
        "for path in possible_paths:\n",
        "    if os.path.exists(path):\n",
        "        csv_file = path\n",
        "        print(f\"✓ Found dataset at: {csv_file}\")\n",
        "        break\n",
        "\n",
        "if csv_file is None:\n",
        "    print(f\"✗ Dataset.csv not found in any of these locations:\")\n",
        "    for p in possible_paths:\n",
        "        print(f\"  - {p}\")\n",
        "    print(\"\\nSearching for any .csv files in current dir and home...\")\n",
        "    for root, dirs, files in os.walk(os.getcwd()):\n",
        "        for file in files:\n",
        "            if file.endswith('.csv'):\n",
        "                print(f\"  Found: {os.path.join(root, file)}\")\n",
        "        break  # only check top level\n",
        "    df_full = None\n",
        "else:\n",
        "    df_full = pd.read_csv(csv_file)\n",
        "    print(f\"✓ Loaded Dataset: {df_full.shape[0]} rows, {df_full.shape[1]} columns\")\n",
        "    print(f\"Columns: {df_full.columns.tolist()[:10]}...\")  # Show first 10 column names\n",
        "    print(f\"First 3 rows:\\n{df_full.head(3)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape: (1552210, 43)\n",
            "✓ Features: 41, Label: SepsisLabel\n",
            "⏳ Imputing missing values per-patient...\n",
            "✓ Imputation complete. NaNs remaining: 0\n",
            "⏳ Scaling features per-patient...\n",
            "✓ Imputation complete. NaNs remaining: 0\n",
            "⏳ Scaling features per-patient...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_72357/4221186414.py:63: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[-1.69222822 -1.61164593 -1.53106363 -1.45048134 -1.36989904 -1.28931674\n",
            " -1.20873445 -1.12815215 -1.04756985 -0.96698756 -0.88640526 -0.80582296\n",
            " -0.72524067 -0.64465837 -0.56407607 -0.48349378 -0.40291148 -0.32232919\n",
            " -0.24174689 -0.16116459 -0.0805823   0.          0.0805823   0.16116459\n",
            "  0.24174689  0.32232919  0.40291148  0.48349378  0.56407607  0.64465837\n",
            "  0.72524067  0.80582296  0.88640526  0.96698756  1.04756985  1.12815215\n",
            "  1.20873445  1.28931674  1.36989904  1.45048134  1.53106363  1.61164593\n",
            "  1.69222822]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
            "  df_scaled.loc[mask, numeric_cols] = scaler.fit_transform(df_scaled.loc[mask, numeric_cols])\n",
            "/tmp/ipykernel_72357/4221186414.py:63: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[-1.69222822 -1.61164593 -1.53106363 -1.45048134 -1.36989904 -1.28931674\n",
            " -1.20873445 -1.12815215 -1.04756985 -0.96698756 -0.88640526 -0.80582296\n",
            " -0.72524067 -0.64465837 -0.56407607 -0.48349378 -0.40291148 -0.32232919\n",
            " -0.24174689 -0.16116459 -0.0805823   0.          0.0805823   0.16116459\n",
            "  0.24174689  0.32232919  0.40291148  0.48349378  0.56407607  0.64465837\n",
            "  0.72524067  0.80582296  0.88640526  0.96698756  1.04756985  1.12815215\n",
            "  1.20873445  1.28931674  1.36989904  1.45048134  1.53106363  1.61164593\n",
            "  1.69222822]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
            "  df_scaled.loc[mask, numeric_cols] = scaler.fit_transform(df_scaled.loc[mask, numeric_cols])\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Scaling complete.\n"
          ]
        }
      ],
      "source": [
        "# Data preprocessing: per-patient imputation and scaling\n",
        "if df_full is None:\n",
        "    print(\"\\n⚠ Creating synthetic dataset...\")\n",
        "    n_patients = 100\n",
        "    max_time_steps = 200\n",
        "    n_features = 15\n",
        "    data_list = []\n",
        "    for p_id in range(n_patients):\n",
        "        n_steps = np.random.randint(50, max_time_steps)\n",
        "        for t in range(n_steps):\n",
        "            features = np.random.randn(n_features) * 0.5\n",
        "            has_sepsis = np.random.rand() > 0.7\n",
        "            if has_sepsis:\n",
        "                features[:3] += t / max_time_steps * 2\n",
        "            sepsis_label = 1 if (has_sepsis and t > n_steps * 0.6) else 0\n",
        "            data_list.append({'Patient_ID': p_id, **{f'feature_{i}': features[i] for i in range(n_features)}, 'SepsisLabel': sepsis_label})\n",
        "    df_full = pd.DataFrame(data_list)\n",
        "    print(f\"✓ Synthetic dataset: {df_full.shape[0]} rows, {df_full.shape[1]} columns\")\n",
        "\n",
        "if df_full is not None:\n",
        "    # Drop unnamed index and standardize patient ID column\n",
        "    if 'Unnamed: 0' in df_full.columns:\n",
        "        df_full = df_full.drop(columns=['Unnamed: 0'])\n",
        "    if 'Patient_ID' in df_full.columns:\n",
        "        df_full.rename(columns={'Patient_ID': 'patient_id'}, inplace=True)\n",
        "    \n",
        "    print(f\"Shape: {df_full.shape}\")\n",
        "    \n",
        "    # Get numeric feature columns (exclude patient_id and SepsisLabel)\n",
        "    df_imputed = df_full.copy()\n",
        "    numeric_cols = df_imputed.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    if 'patient_id' in numeric_cols:\n",
        "        numeric_cols.remove('patient_id')\n",
        "    if 'SepsisLabel' in numeric_cols:\n",
        "        numeric_cols.remove('SepsisLabel')\n",
        "    \n",
        "    print(f\"✓ Features: {len(numeric_cols)}, Label: SepsisLabel\")\n",
        "    \n",
        "    # Per-patient imputation: forward-fill, backward-fill, then patient mean\n",
        "    print(\"⏳ Imputing missing values per-patient...\")\n",
        "    for col in numeric_cols:\n",
        "        # Forward-fill within each patient group\n",
        "        df_imputed[col] = df_imputed.groupby('patient_id')[col].transform(\n",
        "            lambda x: x.ffill().bfill()\n",
        "        )\n",
        "        # Fill remaining NaNs with patient-specific mean\n",
        "        df_imputed[col] = df_imputed.groupby('patient_id')[col].transform(\n",
        "            lambda x: x.fillna(x.mean())\n",
        "        )\n",
        "        # Fill any remaining NaNs with global mean\n",
        "        df_imputed[col] = df_imputed[col].fillna(df_imputed[col].mean())\n",
        "    \n",
        "    print(f\"✓ Imputation complete. NaNs remaining: {df_imputed[numeric_cols].isna().sum().sum()}\")\n",
        "    \n",
        "    # Per-patient scaling (StandardScaler on each patient's data)\n",
        "    print(\"⏳ Scaling features per-patient...\")\n",
        "    df_scaled = df_imputed.copy()\n",
        "    scaler_dict = {}\n",
        "    \n",
        "    for patient_id in df_scaled['patient_id'].unique():\n",
        "        mask = df_scaled['patient_id'] == patient_id\n",
        "        scaler = StandardScaler()\n",
        "        df_scaled.loc[mask, numeric_cols] = scaler.fit_transform(df_scaled.loc[mask, numeric_cols])\n",
        "        scaler_dict[patient_id] = scaler\n",
        "    \n",
        "    print(\"✓ Scaling complete.\")\n",
        "else:\n",
        "    print(\"ERROR: df_full is None.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sequence data created: X_seq (40336, 256, 41), y_seq (40336, 256)\n",
            "Train: (32268, 256, 41), Val: (4033, 256, 41), Test: (4035, 256, 41)\n",
            "Train: (32268, 256, 41), Val: (4033, 256, 41), Test: (4035, 256, 41)\n"
          ]
        }
      ],
      "source": [
        "# Sequence creation: variable-length sequences per patient, padded post-padding to max_seq_len\n",
        "if df_full is not None:\n",
        "    # Extract sequences per patient\n",
        "    max_seq_len = 256\n",
        "    \n",
        "    X_seq_list = []\n",
        "    y_seq_list = []\n",
        "    patient_ids = []\n",
        "    \n",
        "    for patient_id in sorted(df_scaled['patient_id'].unique()):\n",
        "        mask = df_scaled['patient_id'] == patient_id\n",
        "        X_pat = df_scaled.loc[mask, numeric_cols].values  # shape: (time_steps, features)\n",
        "        y_pat = df_scaled.loc[mask, 'SepsisLabel'].values if 'SepsisLabel' in df_scaled.columns else np.ones(mask.sum())  # shape: (time_steps,)\n",
        "        \n",
        "        # Post-pad or truncate to max_seq_len\n",
        "        if len(X_pat) > max_seq_len:\n",
        "            X_pat = X_pat[-max_seq_len:, :]\n",
        "            y_pat = y_pat[-max_seq_len:]\n",
        "        else:\n",
        "            pad_len = max_seq_len - len(X_pat)\n",
        "            X_pat = np.vstack([X_pat, np.zeros((pad_len, X_pat.shape[1]))])\n",
        "            y_pat = np.concatenate([y_pat, np.zeros(pad_len)])\n",
        "        \n",
        "        X_seq_list.append(X_pat)\n",
        "        y_seq_list.append(y_pat)\n",
        "        patient_ids.append(patient_id)\n",
        "    \n",
        "    X_seq = np.array(X_seq_list)  # shape: (n_patients, max_seq_len, n_features)\n",
        "    y_seq = np.array(y_seq_list)  # shape: (n_patients, max_seq_len)\n",
        "    \n",
        "    print(f\"Sequence data created: X_seq {X_seq.shape}, y_seq {y_seq.shape}\")\n",
        "    \n",
        "    # Train/val/test split (80/10/10)\n",
        "    n = len(X_seq)\n",
        "    idx = np.arange(n)\n",
        "    np.random.shuffle(idx)\n",
        "    n_train = int(0.8 * n)\n",
        "    n_val = int(0.1 * n)\n",
        "    \n",
        "    X_train_seq = X_seq[idx[:n_train]]\n",
        "    y_train_seq = y_seq[idx[:n_train]]\n",
        "    X_val_seq = X_seq[idx[n_train:n_train+n_val]]\n",
        "    y_val_seq = y_seq[idx[n_train:n_train+n_val]]\n",
        "    X_test_seq = X_seq[idx[n_train+n_val:]]\n",
        "    y_test_seq = y_seq[idx[n_train+n_val:]]\n",
        "    \n",
        "    # Also keep unpadded sequences for test set (for PhysioNet scoring)\n",
        "    y_test_list = [y_seq_list[i][:np.sum(y_seq_list[i] != 0) + 1] if np.sum(y_seq_list[i] != 0) > 0 else y_seq_list[i] for i in range(len(patient_ids))]\n",
        "    \n",
        "    print(f\"Train: {X_train_seq.shape}, Val: {X_val_seq.shape}, Test: {X_test_seq.shape}\")\n",
        "else:\n",
        "    print(\"ERROR: Cannot create sequences. df_full is None.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluation functions (MUST COME BEFORE TUNING CELL)\n",
        "def plot_confusion_matrix(cm, classes=['Neg','Pos'], title='Confusion matrix', cmap=plt.cm.Blues):\n",
        "    \"\"\"Plot confusion matrix.\"\"\"\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "    \n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in np.ndindex(cm.shape):\n",
        "        plt.text(j, i, format(cm[i, j], 'd'),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "    \n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.tight_layout()\n",
        "\n",
        "def evaluate_sequence_model(model, X_seq, y_seq, threshold=0.5, name='model'):\n",
        "    \"\"\"Evaluate sequence model on flattened predictions.\"\"\"\n",
        "    preds = model.predict(X_seq, verbose=0)\n",
        "    preds_flat = preds.reshape(-1)\n",
        "    y_flat = y_seq.reshape(-1)\n",
        "    \n",
        "    auc = roc_auc_score(y_flat, preds_flat)\n",
        "    avg_prec = average_precision_score(y_flat, preds_flat)\n",
        "    y_pred_bin = (preds_flat >= threshold).astype(int)\n",
        "    prec = precision_score(y_flat, y_pred_bin, zero_division=0)\n",
        "    rec = recall_score(y_flat, y_pred_bin, zero_division=0)\n",
        "    f1 = f1_score(y_flat, y_pred_bin, zero_division=0)\n",
        "    cm = confusion_matrix(y_flat, y_pred_bin)\n",
        "    \n",
        "    print(f\"Evaluation for {name}:\")\n",
        "    print(f\"  AUC: {auc:.4f}, AP: {avg_prec:.4f}\")\n",
        "    print(f\"  Precision: {prec:.4f}, Recall: {rec:.4f}, F1: {f1:.4f}\")\n",
        "    print('  Confusion matrix:\\n', cm)\n",
        "    \n",
        "    fpr, tpr, _ = roc_curve(y_flat, preds_flat)\n",
        "    precision_vals, recall_vals, _ = precision_recall_curve(y_flat, preds_flat)\n",
        "    \n",
        "    plt.figure(figsize=(14,4))\n",
        "    plt.subplot(1,3,1)\n",
        "    plt.plot(fpr, tpr, label=f'AUC={auc:.3f}')\n",
        "    plt.plot([0,1],[0,1],'k--')\n",
        "    plt.xlabel('FPR')\n",
        "    plt.ylabel('TPR')\n",
        "    plt.title(f'ROC Curve ({name})')\n",
        "    plt.legend()\n",
        "    \n",
        "    plt.subplot(1,3,2)\n",
        "    plt.plot(recall_vals, precision_vals, label=f'AP={avg_prec:.3f}')\n",
        "    plt.xlabel('Recall')\n",
        "    plt.ylabel('Precision')\n",
        "    plt.title(f'Precision-Recall ({name})')\n",
        "    plt.legend()\n",
        "    \n",
        "    plt.subplot(1,3,3)\n",
        "    plot_confusion_matrix(cm, classes=['Neg','Pos'], title=f'Confusion Matrix ({name})')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    return {'auc': auc, 'ap': avg_prec, 'precision': prec, 'recall': rec, 'f1': f1, 'confusion_matrix': cm}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-17 05:11:03.886874: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-12-17 05:11:03.950146: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1765944663.961409    5258 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1765944663.965820    5258 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1765944664.013288    5258 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765944664.013306    5258 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765944664.013308    5258 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765944664.013309    5258 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-17 05:11:04.018895: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX512F AVX512_VNNI AVX512_BF16, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'X_train_seq' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01moptimizers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Adam\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AUC, Precision, Recall\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m input_shape = (\u001b[43mX_train_seq\u001b[49m.shape[\u001b[32m1\u001b[39m], X_train_seq.shape[\u001b[32m2\u001b[39m])\n\u001b[32m      8\u001b[39m os.makedirs(\u001b[33m'\u001b[39m\u001b[33m/tmp/icu_tune\u001b[39m\u001b[33m'\u001b[39m, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_optimizer\u001b[39m(opt_name, lr):\n",
            "\u001b[31mNameError\u001b[39m: name 'X_train_seq' is not defined"
          ]
        }
      ],
      "source": [
        "# Model builders for two-stage tuning\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import SimpleRNN, LSTM, Conv1D, TimeDistributed, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.metrics import AUC, Precision, Recall\n",
        "\n",
        "input_shape = (X_train_seq.shape[1], X_train_seq.shape[2])\n",
        "os.makedirs('/tmp/icu_tune', exist_ok=True)\n",
        "\n",
        "def get_optimizer(opt_name, lr):\n",
        "    \"\"\"Get optimizer by name.\"\"\"\n",
        "    if (opt_name or '').lower() == 'adam':\n",
        "        return Adam(lr)\n",
        "    return Adam(lr)\n",
        "\n",
        "def map_output_to_prob(preds, final_act):\n",
        "    \"\"\"Map model outputs to probability-like [0,1] range based on activation.\"\"\"\n",
        "    preds = np.asarray(preds)\n",
        "    if final_act == 'tanh':\n",
        "        return (preds + 1.0) / 2.0\n",
        "    if final_act == 'relu':\n",
        "        return np.clip(preds, 0.0, 1.0)\n",
        "    return preds\n",
        "\n",
        "def safe_roc_auc(y_true_flat, pred_flat):\n",
        "    \"\"\"Safe AUC computation.\"\"\"\n",
        "    try:\n",
        "        return float(roc_auc_score(y_true_flat, pred_flat))\n",
        "    except Exception:\n",
        "        return float('nan')\n",
        "\n",
        "def build_rnn_model(hp):\n",
        "    \"\"\"Build RNN model with hyperparams hp.\"\"\"\n",
        "    final_act = hp.get('final_activation', 'sigmoid')\n",
        "    loss_fn = hp.get('loss', 'binary_crossentropy')\n",
        "    m = Sequential([\n",
        "        tf.keras.Input(shape=input_shape),\n",
        "        SimpleRNN(hp['units'], return_sequences=True),\n",
        "        Dropout(hp['dropout']),\n",
        "        TimeDistributed(Dense(1, activation=final_act))\n",
        "    ])\n",
        "    opt = get_optimizer(hp.get('optimizer','adam'), hp.get('lr',1e-3))\n",
        "    m.compile(optimizer=opt, loss=loss_fn, metrics=['accuracy', Precision(), Recall(), AUC(name='auc')])\n",
        "    return m\n",
        "\n",
        "def build_cnn_model(hp):\n",
        "    \"\"\"Build CNN model with hyperparams hp.\"\"\"\n",
        "    final_act = hp.get('final_activation', 'sigmoid')\n",
        "    loss_fn = hp.get('loss', 'binary_crossentropy')\n",
        "    m = Sequential([\n",
        "        tf.keras.Input(shape=input_shape),\n",
        "        Conv1D(hp['f1'], 3, activation='relu', padding='same'),\n",
        "        Dropout(hp['dropout']),\n",
        "        Conv1D(hp['f2'], 3, activation='relu', padding='same'),\n",
        "        Dropout(hp['dropout']),\n",
        "        TimeDistributed(Dense(1, activation=final_act))\n",
        "    ])\n",
        "    opt = get_optimizer(hp.get('optimizer','adam'), hp.get('lr',1e-3))\n",
        "    m.compile(optimizer=opt, loss=loss_fn, metrics=['accuracy', Precision(), Recall(), AUC(name='auc')])\n",
        "    return m\n",
        "\n",
        "def build_lgstm_model(hp):\n",
        "    \"\"\"Build stacked LSTM model with hyperparams hp.\"\"\"\n",
        "    final_act = hp.get('final_activation', 'sigmoid')\n",
        "    loss_fn = hp.get('loss', 'binary_crossentropy')\n",
        "    m = Sequential([\n",
        "        tf.keras.Input(shape=input_shape),\n",
        "        LSTM(hp['u1'], return_sequences=True),\n",
        "        Dropout(hp['dropout']),\n",
        "        LSTM(hp['u2'], return_sequences=True),\n",
        "        Dropout(hp['dropout']),\n",
        "        TimeDistributed(Dense(1, activation=final_act))\n",
        "    ])\n",
        "    opt = get_optimizer(hp.get('optimizer','adam'), hp.get('lr',1e-3))\n",
        "    m.compile(optimizer=opt, loss=loss_fn, metrics=['accuracy', Precision(), Recall(), AUC(name='auc')])\n",
        "    return m"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== TWO-STAGE HYPERPARAMETER TUNING ===\n",
            "\n",
            "STAGE 1: Fast model-family selection\n",
            "\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'build_rnn_model' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 89\u001b[39m\n\u001b[32m     86\u001b[39m     df = pd.DataFrame(results).sort_values(\u001b[33m'\u001b[39m\u001b[33mscore\u001b[39m\u001b[33m'\u001b[39m, ascending=\u001b[38;5;28;01mFalse\u001b[39;00m).reset_index(drop=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     87\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m best_hp, df\n\u001b[32m---> \u001b[39m\u001b[32m89\u001b[39m best_rnn_hp, rnn_df_small = grid_search(\u001b[43mbuild_rnn_model\u001b[49m, small_rnn_grid, \u001b[33m'\u001b[39m\u001b[33mRNN\u001b[39m\u001b[33m'\u001b[39m, epochs=\u001b[32m1\u001b[39m)\n\u001b[32m     90\u001b[39m best_cnn_hp, cnn_df_small = grid_search(build_cnn_model, small_cnn_grid, \u001b[33m'\u001b[39m\u001b[33mCNN\u001b[39m\u001b[33m'\u001b[39m, epochs=\u001b[32m1\u001b[39m)\n\u001b[32m     91\u001b[39m best_lgstm_hp, lgstm_df_small = grid_search(build_lgstm_model, small_lgstm_grid, \u001b[33m'\u001b[39m\u001b[33mLGSTM\u001b[39m\u001b[33m'\u001b[39m, epochs=\u001b[32m1\u001b[39m)\n",
            "\u001b[31mNameError\u001b[39m: name 'build_rnn_model' is not defined"
          ]
        }
      ],
      "source": [
        "# STAGE 1: Small grids (fast) to pick best model family\n",
        "print(\"=== TWO-STAGE HYPERPARAMETER TUNING ===\\n\")\n",
        "print(\"STAGE 1: Fast model-family selection\\n\")\n",
        "\n",
        "small_rnn_grid = { \n",
        "    'units':[64], 'dropout':[0.2], 'lr':[1e-3], 'batch_size':[64], \n",
        "    'final_activation':['relu'], 'loss':['binary_crossentropy'], 'optimizer':['adam'] \n",
        "}\n",
        "small_cnn_grid = { \n",
        "    'f1':[32], 'f2':[64], 'dropout':[0.2], 'lr':[1e-3], 'batch_size':[64], \n",
        "    'final_activation':['relu'], 'loss':['binary_crossentropy'], 'optimizer':['adam'] \n",
        "}\n",
        "small_lgstm_grid = { \n",
        "    'u1':[64], 'u2':[32], 'dropout':[0.2], 'lr':[1e-3], 'batch_size':[64], \n",
        "    'final_activation':['relu'], 'loss':['binary_crossentropy'], 'optimizer':['adam'] \n",
        "}\n",
        "\n",
        "# Helper function for scoring\n",
        "def trial_score_from_model(m, hp, X_val, y_val):\n",
        "    \"\"\"Score a model on validation data.\"\"\"\n",
        "    preds = m.predict(X_val, verbose=0)[:,:,0]\n",
        "    probs = map_output_to_prob(preds, hp.get('final_activation','sigmoid'))\n",
        "    return safe_roc_auc(y_val.reshape(-1), probs)\n",
        "\n",
        "def grid_search(build_fn, grid, name, epochs=10, X_train=None, y_train=None, X_val=None, y_val=None):\n",
        "    \"\"\"Run grid search over hyperparameters.\"\"\"\n",
        "    # Use provided data or fall back to globals\n",
        "    if X_train is None:\n",
        "        try:\n",
        "            X_train = globals()['X_train_seq']\n",
        "        except KeyError:\n",
        "            print(f'[{name}] ERROR: X_train_seq not defined. Did you run preprocessing cells first?')\n",
        "            return None, pd.DataFrame()\n",
        "    if y_train is None:\n",
        "        y_train = globals()['y_train_seq']\n",
        "    if X_val is None:\n",
        "        X_val = globals()['X_val_seq']\n",
        "    if y_val is None:\n",
        "        y_val = globals()['y_val_seq']\n",
        "    \n",
        "    # Check if data is empty or invalid\n",
        "    if X_train is None or X_val is None or X_train.shape[0] == 0 or X_val.shape[0] == 0:\n",
        "        print(f'[{name}] Skipping: No training or validation data available')\n",
        "        print(f'  X_train shape: {X_train.shape if X_train is not None else \"None\"}')\n",
        "        print(f'  X_val shape: {X_val.shape if X_val is not None else \"None\"}')\n",
        "        return None, pd.DataFrame()\n",
        "    \n",
        "    results = []\n",
        "    best_hp = None\n",
        "    best_score = None\n",
        "    for hp in ParameterGrid(grid):\n",
        "        print(f'[{name}] trying {hp}')\n",
        "        m = build_fn(hp)\n",
        "        if hp.get('loss') == 'hinge':\n",
        "            # Hinge loss requires {-1, +1} labels\n",
        "            y_train_fit = (y_train * 2.0) - 1.0\n",
        "            y_val_fit = (y_val * 2.0) - 1.0\n",
        "            try:\n",
        "                m.fit(X_train, y_train_fit, validation_data=(X_val, y_val_fit), \n",
        "                      epochs=epochs, batch_size=hp['batch_size'], verbose=0)\n",
        "            except Exception as e:\n",
        "                print('  fit failed:', e)\n",
        "                continue\n",
        "            score = trial_score_from_model(m, hp, X_val, y_val)\n",
        "        else:\n",
        "            try:\n",
        "                hist = m.fit(X_train, y_train, validation_data=(X_val, y_val), \n",
        "                            epochs=epochs, batch_size=hp['batch_size'], verbose=0)\n",
        "            except Exception as e:\n",
        "                print('  fit failed:', e)\n",
        "                continue\n",
        "            h = hist.history\n",
        "            if 'val_auc' in h:\n",
        "                score = max(h['val_auc'])\n",
        "            else:\n",
        "                score = trial_score_from_model(m, hp, X_val, y_val)\n",
        "        results.append({**hp, 'score': score})\n",
        "        print(f'  score={score:.4f}')\n",
        "        if best_score is None or (np.nan_to_num(score) > np.nan_to_num(best_score)):\n",
        "            best_score = score\n",
        "            best_hp = hp\n",
        "            try:\n",
        "                m.save_weights(f'/tmp/icu_tune/{name}_best_weights.h5')\n",
        "            except Exception:\n",
        "                pass\n",
        "    df = pd.DataFrame(results).sort_values('score', ascending=False).reset_index(drop=True)\n",
        "    return best_hp, df\n",
        "\n",
        "best_rnn_hp, rnn_df_small = grid_search(build_rnn_model, small_rnn_grid, 'RNN', epochs=1)\n",
        "best_cnn_hp, cnn_df_small = grid_search(build_cnn_model, small_cnn_grid, 'CNN', epochs=1)\n",
        "best_lgstm_hp, lgstm_df_small = grid_search(build_lgstm_model, small_lgstm_grid, 'LGSTM', epochs=1)\n",
        "\n",
        "# Choose winner\n",
        "stage1_rows = []\n",
        "if rnn_df_small.shape[0]>0: stage1_rows.append(('RNN', rnn_df_small.iloc[0].to_dict()))\n",
        "if cnn_df_small.shape[0]>0: stage1_rows.append(('CNN', cnn_df_small.iloc[0].to_dict()))\n",
        "if lgstm_df_small.shape[0]>0: stage1_rows.append(('LGSTM', lgstm_df_small.iloc[0].to_dict()))\n",
        "best_stage1 = None\n",
        "best_stage1_score = None\n",
        "for name, row in stage1_rows:\n",
        "    s = row.get('score', np.nan)\n",
        "    if best_stage1 is None or (np.nan_to_num(s) > np.nan_to_num(best_stage1_score)):\n",
        "        best_stage1 = name\n",
        "        best_stage1_score = s\n",
        "\n",
        "if best_stage1 is not None:\n",
        "    print(f'\\n✓ Stage-1 winner: {best_stage1} (score={best_stage1_score:.4f})\\n')\n",
        "else:\n",
        "    print('\\n✗ No stage-1 winner (all models failed). Skipping stage-2.\\n')\n",
        "\n",
        "# STAGE 2: Large grids (include relu, hinge, adam)\n",
        "print(\"STAGE 2: Exhaustive search for winner\\n\")\n",
        "big_rnn_grid = { \n",
        "    'units':[64,128], 'dropout':[0.2,0.4], 'lr':[1e-3,1e-4], 'batch_size':[32,64], \n",
        "    'final_activation':['sigmoid','tanh','relu'], 'loss':['binary_crossentropy','mse','hinge'], \n",
        "    'optimizer':['adam'] \n",
        "}\n",
        "big_cnn_grid = { \n",
        "    'f1':[32,64], 'f2':[64,128], 'dropout':[0.2,0.4], 'lr':[1e-3,1e-4], 'batch_size':[32,64], \n",
        "    'final_activation':['sigmoid','tanh','relu'], 'loss':['binary_crossentropy','mse','hinge'], \n",
        "    'optimizer':['adam'] \n",
        "}\n",
        "big_lgstm_grid = { \n",
        "    'u1':[64,128], 'u2':[32,64], 'dropout':[0.2,0.4], 'lr':[1e-3,1e-4], 'batch_size':[32,64], \n",
        "    'final_activation':['sigmoid','tanh','relu'], 'loss':['binary_crossentropy','mse','hinge'], \n",
        "    'optimizer':['adam'] \n",
        "}\n",
        "\n",
        "if best_stage1 == 'RNN':\n",
        "    best_big_hp, big_df = grid_search(build_rnn_model, big_rnn_grid, 'RNN_big', epochs=10)\n",
        "    chosen_builder = build_rnn_model\n",
        "elif best_stage1 == 'CNN':\n",
        "    best_big_hp, big_df = grid_search(build_cnn_model, big_cnn_grid, 'CNN_big', epochs=10)\n",
        "    chosen_builder = build_cnn_model\n",
        "elif best_stage1 == 'LGSTM':\n",
        "    best_big_hp, big_df = grid_search(build_lgstm_model, big_lgstm_grid, 'LGSTM_big', epochs=10)\n",
        "    chosen_builder = build_lgstm_model\n",
        "else:\n",
        "    best_big_hp, big_df = None, pd.DataFrame()\n",
        "    chosen_builder = None\n",
        "\n",
        "# Save stage results\n",
        "rnn_df_small.to_csv('/tmp/icu_tune/rnn_stage1.csv', index=False)\n",
        "cnn_df_small.to_csv('/tmp/icu_tune/cnn_stage1.csv', index=False)\n",
        "lgstm_df_small.to_csv('/tmp/icu_tune/lgstm_stage1.csv', index=False)\n",
        "big_df.to_csv('/tmp/icu_tune/big_grid_results.csv', index=False)\n",
        "\n",
        "if not big_df.empty:\n",
        "    plt.figure(figsize=(10,6))\n",
        "    sns.barplot(data=big_df.head(8), x='score', y=big_df.head(8).index)\n",
        "    plt.title(f'{best_stage1} - Top 8 configs (stage 2)')\n",
        "    plt.xlabel('Score (val ROC AUC)')\n",
        "    plt.ylabel('Config rank')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print('No big-grid results to plot.')\n",
        "\n",
        "print(f'\\n✓ Stage-2 best config: {best_big_hp}' if best_big_hp else '\\n✗ No stage-2 results')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No best_big_hp found; skipping final experiments.\n"
          ]
        }
      ],
      "source": [
        "# Final experiments: train best found for multiple epochs\n",
        "if best_big_hp is not None:\n",
        "    print(\"\\n=== FINAL EXPERIMENTS ===\\n\")\n",
        "    final_epochs = [50,100,200]\n",
        "    final_results = {}\n",
        "    \n",
        "    def safe_build_and_load(build_fn, hp, name):\n",
        "        m = build_fn(hp)\n",
        "        try:\n",
        "            m.load_weights(f'/tmp/icu_tune/{name}_best_weights.h5')\n",
        "            print(f'  loaded warm-start weights for {name}')\n",
        "        except Exception:\n",
        "            pass\n",
        "        return m\n",
        "    \n",
        "    for e in final_epochs:\n",
        "        print(f'Training {best_stage1} for {e} epochs')\n",
        "        m = safe_build_and_load(chosen_builder, best_big_hp, f'{best_stage1}_big')\n",
        "        if best_big_hp.get('loss') == 'hinge':\n",
        "            y_train_fit = (y_train_seq * 2.0) - 1.0\n",
        "            y_val_fit = (y_val_seq * 2.0) - 1.0\n",
        "            hist = m.fit(X_train_seq, y_train_fit, validation_data=(X_val_seq, y_val_fit), \n",
        "                        epochs=e, batch_size=best_big_hp['batch_size'], verbose=1)\n",
        "        else:\n",
        "            hist = m.fit(X_train_seq, y_train_seq, validation_data=(X_val_seq, y_val_seq), \n",
        "                        epochs=e, batch_size=best_big_hp['batch_size'], verbose=1)\n",
        "        met = evaluate_sequence_model(m, X_test_seq, y_test_seq, threshold=0.5, \n",
        "                                      name=f'{best_stage1}_final_e{e}')\n",
        "        final_results[e] = {'history': hist.history, 'metrics': met}\n",
        "        try:\n",
        "            m.save_weights(f'./{best_stage1}_final_e{e}_weights.h5')\n",
        "        except Exception:\n",
        "            pass\n",
        "    \n",
        "    # Plot final AUC vs epochs\n",
        "    plt.figure(figsize=(8,5))\n",
        "    xs = sorted(final_results.keys())\n",
        "    ys = [final_results[x]['metrics']['auc'] for x in xs]\n",
        "    plt.plot(xs, ys, marker='o', linewidth=2, markersize=8)\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Test AUC')\n",
        "    plt.title(f'{best_stage1} - Final Model: Test AUC vs Epochs')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Save summary\n",
        "    rows = []\n",
        "    for e, d in final_results.items():\n",
        "        rows.append({\n",
        "            'model': best_stage1, 'epochs': e, 'auc': d['metrics']['auc'], \n",
        "            'ap': d['metrics']['ap'], 'precision': d['metrics']['precision'], \n",
        "            'recall': d['metrics']['recall'], 'f1': d['metrics']['f1']\n",
        "        })\n",
        "    pd.DataFrame(rows).to_csv('/tmp/icu_tune/final_experiments_summary.csv', index=False)\n",
        "    print('\\nTwo-stage tuning + final experiments complete. Results in /tmp/icu_tune/')\n",
        "else:\n",
        "    print(\"No best_big_hp found; skipping final experiments.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "AI-Health",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
